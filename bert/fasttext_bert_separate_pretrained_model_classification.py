# -*- coding: utf-8 -*-
"""fasttext_bert_separate_pretrained_model_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1mT_BxtTJg7vDL-IscykjcELJmyNsKG

# Install Required Packages
"""

import os

# Disable asynchronously kernel launches which is useful for debugging.
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# ! pip install datasets transformers numpy sklearn

"""# Gather Data"""

import pandas as pd

train_dataset_df = pd.read_csv("/content/drive/MyDrive/Datasets/fasttext_train_dataset.csv")
test_dataset_df = pd.read_csv("/content/drive/MyDrive/Datasets/fasttext_test_dataset.csv")


"""# Prepare Data"""

from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from datasets import DatasetDict, Dataset


# Replace with vectores
# Target 4 out range error: Should it begin from 0?
label_mapping = {'Sport': 0, 'Politics': 1, 'Economy': 2, 'Social': 3}

# Clean and format datasets and store them all in a DatasetDict.
train_dataset_df.dropna(how="any", inplace=True)
#train_dataset_df["label"].astype(str).replace(label_mapping, inplace=True)
train_dataset_df, validation_dataset_df = train_test_split(train_dataset_df, test_size=0.2, random_state=42, shuffle=True)
test_dataset_df.dropna(how="any", inplace=True)
#test_dataset_df["label"].astype(str).replace(label_mapping, inplace=True)
raw_datasets = DatasetDict({
    "train": Dataset.from_pandas(train_dataset_df),
    "validation": Dataset.from_pandas(validation_dataset_df),
    "test": Dataset.from_pandas(test_dataset_df),
})

# print('Train dataset after cleaning: ', train_dataset_df)

# Preprocess the datasets by tokenization:
tokenizer = AutoTokenizer.from_pretrained(
    "HooshvareLab/bert-fa-base-uncased",
)

# @TODO Padding should be min(maximum length of samples, 512)
def preprocess_documents(documents):
    # The maximum sequence length in BERT model is 512.
    tokenized_documents = tokenizer(documents["text"], padding="max_length", truncation=True, max_length=512)
    tokenized_documents["label"] = [label_mapping[label] for label in documents["label"]]

    return tokenized_documents


tokenized_datasets = raw_datasets.map(preprocess_documents, batched=True)

# Divide dataset into three subset for training, validation and testing.
train_dataset = tokenized_datasets["train"]
validation_dataset = tokenized_datasets["validation"]
test_dataset = tokenized_datasets["test"]


print("An example of a sample training documents after preprocessing:")
for i in range(0, 10):
  print("Label: ", train_dataset['label'][i], ", Text: ", train_dataset['text'][i])

"""# Define and Train Model"""

from transformers import AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer
import sys

output_dir = "/content/drive/MyDrive/nlp_output_dir/fasttext_classification"
# model_name_or_path = "/content/drive/MyDrive/nlp_output_dir/fasttext_classification"
model_name_or_path = "HooshvareLab/bert-fa-base-uncased"

labels = raw_datasets["train"].unique("label")


# Pretrain our BERT model, we can pass custom config parameters directly or by a AutoConfig option.
# config = AutoConfig.from_pretrained(
#     model_name_or_path,
#     num_labels=len(labels),
# )
model = AutoModelForSequenceClassification.from_pretrained(
    model_name_or_path,
    num_labels=len(label_mapping)
)
print('\n---Model Architectures: ', model.config.architectures)


# Create a trainer from our pre-trained model to fine tune it.
training_args = TrainingArguments(output_dir=output_dir, save_total_limit=6, load_best_model_at_end=True, save_strategy="epoch", evaluation_strategy="epoch")
trainer = Trainer(
    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=validation_dataset
)

# Fune tuning params using Validation
# num_attention_heads, vocab_size, num_hidden_layers, hidden_size, initializer_range

# Fine tune our model
trainer.train()

"""Save Model"""

trainer.save_model()

"""Evaluate Model"""

import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(output_dir= output_dir, save_total_limit=6, load_best_model_at_end=True, save_strategy="epoch", evaluation_strategy="epoch")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)
trainer.evaluate()

import numpy as np
import csv

# Removing the `label` columns because it contains -1 and Trainer won't like that.
predict_dataset = test_dataset.remove_columns("label")

predictions = trainer.predict(predict_dataset, metric_key_prefix="predict").predictions
predictions = np.argmax(predictions, axis=1)



output_predict_file = os.path.join(training_args.output_dir, f"predict_results.csv")
text_label_predictions = []
for index, prediction_label in enumerate(predictions):
    label_name = labels[prediction_label]
    text_label_predictions.append({
        "text": predict_dataset['text'][index],
        "label": label_name
    })

with open(output_predict_file, "w") as file:
    writer = csv.DictWriter(file, fieldnames=["text", "label"])
    writer.writeheader()
    writer.writerows(text_label_predictions)

print(text_label_predictions)